{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading libraries for importing and cleaning data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "#training\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "path = \"/Users/hangxin/Documents/GitHub/Coupon_purchase/coupon-purchase-prediction/\"\n",
    "all_files = glob.glob(path + '/*.csv')\n",
    "file_dict = {}\n",
    "for file in all_files:\n",
    "    file_dict[os.path.basename(file)[:-4]] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to view the first three rows of each dataframe\n",
    "for file,df in file_dict.items():\n",
    "    print('The data frame is ',file)\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_dict['user_list'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender counts\n",
    "sns.set_style('white')\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.countplot(x='SEX_ID',data=file_dict['user_list'])\n",
    "sns.despine()\n",
    "plt.title('Gender Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are more male users than female users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age distribution\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=file_dict['user_list'], x='AGE',kde=True,bins=20,hue='SEX_ID')\n",
    "sns.despine()\n",
    "plt.title('User Age Distrition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot suggests that most users are aged between 28 - 58 years old. \n",
    "\n",
    "For users younger than 25 years old, the numbers of female and male users are about the same. There are more female users aged between 25 to 45. In contrast, there are more male users older than 45.\n",
    "\n",
    "Before exploring coupon lists for training and test, I'm going to translate the capsule and genre of each coupon from Japanese to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_translation(df_input):\n",
    "    df = df_input.copy(deep=True)\n",
    "    #read in CAPSULE_TEXT_Translation file from documentation folder to translate Japanese to English\n",
    "    capsule_text_translation1_path = '/Users/hangxin/Documents/GitHub/Coupon_purchase/coupon-purchase-prediction/documentation/capsule_text_translation1.xlsx'\n",
    "    genre_name_translation_path = '/Users/hangxin/Documents/GitHub/Coupon_purchase/coupon-purchase-prediction/documentation/genre_name_translation.xlsx'\n",
    "    \n",
    "    capsule_text_translation1 = pd.read_excel(capsule_text_translation1_path,engine='openpyxl')\n",
    "    genre_name_translation = pd.read_excel(genre_name_translation_path, engine='openpyxl')\n",
    "    \n",
    "    capsule_trans_df = dict(zip(capsule_text_translation1['CAPSULE_TEXT'],capsule_text_translation1['English Translation']))\n",
    "    genre_trans_df = dict(zip(genre_name_translation['CAPSULE_TEXT'],genre_name_translation['English Translation']))\n",
    "    \n",
    "    #translate capsule text and genre to English\n",
    "    df['capsule_text_eng'] = df['CAPSULE_TEXT'].apply(lambda x: capsule_trans_df[x])\n",
    "    df['genre_name_eng'] = df['GENRE_NAME'].apply(lambda x: genre_trans_df[x])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_coupon_list_train = data_translation(file_dict['coupon_list_train'])\n",
    "eng_coupon_list_test = data_translation(file_dict['coupon_list_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count the number of coupons usable on each day of week and holiday\n",
    "usable_date_cnts = {}\n",
    "for col in eng_coupon_list_train.iloc[:,11:20].columns:\n",
    "    #values in these usable dates columns are 0,1,2\n",
    "    #no further info provided for the meaning of 2, I assume it is usable\n",
    "    usable_date_cnts[col] = len(np.where(eng_coupon_list_train[col]>0)[0])\n",
    "\n",
    "#visualize the counts of usable days\n",
    "keys = list(usable_date_cnts.keys())\n",
    "vals = list(usable_date_cnts.values())\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "ax = sns.barplot(x=keys,y=vals, palette='crest')\n",
    "ax.set_xticklabels(keys, rotation=45)\n",
    "plt.title('Usable Dates of Coupons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coupons are usable mainly from Wednesday to Friday. Weekends are the least usable, followed by holidays and the day before. This makes sense as businesses that are having healthy sales over the weekends or holidays, aim to increase sales on weekdays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the distribution of discount rates\n",
    "plt.figure(figsize=(7,5))\n",
    "ax = sns.histplot(eng_coupon_list_train['PRICE_RATE'],bins=60,color='lightblue')\n",
    "ax.set_xlim([50,100]) #very few data points are on the left side of 50 \n",
    "sns.despine()\n",
    "\n",
    "plt.title('Distribution of Discount Rates')\n",
    "plt.xlabel('Discount Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most discount rates are 50% of the original prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize the distribution of original prices in Japanese yen\n",
    "plt.figure(figsize=(7,5))\n",
    "ax = sns.histplot(eng_coupon_list_train['CATALOG_PRICE'],\n",
    "                  bins=200,color='lightblue')\n",
    "ax.set_xlim([0,175000])\n",
    "sns.despine()\n",
    "\n",
    "plt.title('Distribution of Original Prices')\n",
    "plt.xlabel('Original Price in Janpanese Yen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joint plot to compare the origianl price and discount price\n",
    "sns.jointplot(x='CATALOG_PRICE',y='DISCOUNT_PRICE',\n",
    "              data=eng_coupon_list_train,height=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This joint plot compares the original catalog price to the discounted price. The largest graph indicates that outliers exist for the prices, while most prices are under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#visualization of the prefecture names of shops\n",
    "geometry = [Point(xy) for xy in zip(file_dict['prefecture_locations']['LONGITUDE'], file_dict['prefecture_locations']['LATITUDE'])]\n",
    "gdf = GeoDataFrame(file_dict['prefecture_locations'], geometry=geometry) \n",
    "gdf = gdf.set_index('PREF_NAME').join(eng_coupon_list_train['ken_name'].value_counts())\n",
    "gdf['ken_name_size'] = gdf['ken_name']/10\n",
    "\n",
    "jp_map = gpd.read_file('/Users/hangxin/Documents/GitHub/Coupon_purchase/jpn_adm_2019_shp/jpn_admbndl_ALL_2019.shp')\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(15,15))\n",
    "\n",
    "jp_map.plot(ax=ax,alpha=0.4, color='grey')\n",
    "gdf.plot(ax=ax, color='blue', marker='o',markersize='ken_name_size')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest blue spot indicates the largest number of coupons are from shops in Tokyo area. The second largest blue spot, southwest to Tokyo, suggests many other coupons are from shops in Osaka and Kyoto area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "**User list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict['user_list'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above information of each column indicates some missing values in \"PREF_NAME\"(residential prefecture) and \"WITHDRAW_DATE\"(unregistered date). So I will add an \"Unknown\" to rows missing \"PREF_NAME\". If \"WITHDRAW_DATE\" is not null, this user unregistered from the website and will not be included in the prediction process.  \n",
    "\n",
    "Also the \"REG_DATE\"(registered date) needs to be changed to date time format. Gender will be converted to categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping users who unregistered from the website\n",
    "user_list_filtered = file_dict['user_list'].iloc[np.where(file_dict['user_list']['WITHDRAW_DATE'].isna())]\n",
    "\n",
    "#drop WITHDRAW_DATE column as it is no longer useful\n",
    "user_list_filtered = user_list_filtered.drop(columns='WITHDRAW_DATE')\n",
    "\n",
    "#change null values to \"Unknown\" in PREF_NAME \n",
    "user_list_filtered['PREF_NAME'] = user_list_filtered['PREF_NAME'].fillna('Unknown')\n",
    "\n",
    "#change REG_DATE to datetime formate\n",
    "user_list_filtered['REG_DATE'] = user_list_filtered['REG_DATE'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#create a column representing how long this user has been registered from the end of the training period\n",
    "training_period = datetime.strptime('2012-06-23 11:59:00', '%Y-%m-%d %H:%M:%S')\n",
    "user_list_filtered['num_year_registered'] = user_list_filtered['REG_DATE'].apply(lambda d:training_period.year-d.year)\n",
    "\n",
    "#convert gender from string to categorical\n",
    "user_list_filtered['SEX_ID'] = user_list_filtered['SEX_ID'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coupon lists**\n",
    "\n",
    "Now I have a cleaned user list. I will also clean the coupon lists for training and test respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_coupon_list_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary of the coupon list for training, there are some null values for validity and usable dates and period. I want to find out why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null valid from dates by capsule:')\n",
    "print(eng_coupon_list_train[eng_coupon_list_train['VALIDFROM'].isna()]['capsule_text_eng'].value_counts())\n",
    "print('\\n')\n",
    "\n",
    "print('Null valid end dates by capsule:')\n",
    "print(eng_coupon_list_train[eng_coupon_list_train['VALIDEND'].isna()]['capsule_text_eng'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null valid from dates by capsule in test set:')\n",
    "print(eng_coupon_list_test[eng_coupon_list_test['VALIDFROM'].isna()]['capsule_text_eng'].value_counts())\n",
    "print('\\n')\n",
    "\n",
    "print('Null valid end dates by capsule in test set:')\n",
    "print(eng_coupon_list_test[eng_coupon_list_test['VALIDEND'].isna()]['capsule_text_eng'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null usable_date_mon by capsule in training set:')\n",
    "print(eng_coupon_list_train[eng_coupon_list_train['USABLE_DATE_MON'].isna()]['capsule_text_eng'].value_counts())\n",
    "print('\\n')\n",
    "\n",
    "print('Null usable_date_mon by capsule in test set:')\n",
    "print(eng_coupon_list_test[eng_coupon_list_test['USABLE_DATE_MON'].isna()]['capsule_text_eng'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The above results indicate the null values for validity dates and usable days are mainly for delivery services, lessons and gift cards, which are not time sensitive. Unlike food services, hotels or hair salons, the above services do not typically show heavier weekend or holiday traffic. So I will assume coupons from these services are valid from the release date to the end of training or test set period. And I will update the valid from and valid end dates correspondingly. Also I will assume these coupons are usable all days of week, including holidays and the day before, and update all usable dates to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_str(date_str, h_m_s):\n",
    "    if h_m_s:\n",
    "        new_date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        new_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    return new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dates(df_input, type_of_dataset):\n",
    "    df = df_input.copy(deep=True)\n",
    "    \n",
    "    #training and test period\n",
    "    #training_start = datetime.strptime('2011-07-01' 12:00:00, '%Y-%m-%d %H:%M:%S')\n",
    "    training_end = datetime.strptime('2012-06-23', '%Y-%m-%d')\n",
    "\n",
    "    #test_start = datetime.strptime('2012-06-24', '%Y-%m-%d')\n",
    "    test_end = datetime.strptime('2012-06-30', '%Y-%m-%d')\n",
    "\n",
    "    #update valid start dates\n",
    "    df['VALIDFROM'] = df.apply(lambda x: parse_date_str(x['DISPFROM'],True).date() if isinstance(x['VALIDFROM'], float) else x['VALIDFROM'], axis=1)\n",
    "          \n",
    "    #update end dates\n",
    "    if type_of_dataset == 'train':\n",
    "        #fill null values with training end date\n",
    "        df['VALIDEND'] = df['VALIDEND'].fillna(training_end)\n",
    "        \n",
    "    if type_of_dataset == 'test':\n",
    "        #fill null values with test end dates \n",
    "        df['VALIDEND'] = df['VALIDEND'].fillna(test_end)\n",
    "    \n",
    "    #update date type\n",
    "    dispDates_cols = ['DISPFROM','DISPEND']\n",
    "    for col in dispDates_cols:\n",
    "        df[col] = df[col].apply(lambda d: parse_date_str(d,True))\n",
    "    \n",
    "    validDates_cols = ['VALIDFROM','VALIDEND']   \n",
    "    for col in validDates_cols:\n",
    "        df[col] = df[col].apply(lambda d: parse_date_str(d, False) if isinstance(d,str) else d)    \n",
    "        \n",
    "    #update valid period  \n",
    "    df['VALIDPERIOD'] = df.apply(lambda x: (x['VALIDEND']-x['VALIDFROM']).days if isinstance(x['VALIDPERIOD'], float) else x['VALIDPERIOD'], axis=1)\n",
    "    df['VALIDPERIOD'] = pd.to_numeric(df['VALIDPERIOD'])\n",
    "    \n",
    "    #update usable days to 1\n",
    "    usable_days = ['USABLE_DATE_MON','USABLE_DATE_TUE','USABLE_DATE_WED','USABLE_DATE_THU',\n",
    "                   'USABLE_DATE_FRI','USABLE_DATE_SAT','USABLE_DATE_SUN',\n",
    "                  'USABLE_DATE_HOLIDAY','USABLE_DATE_BEFORE_HOLIDAY']\n",
    "    for col in usable_days:\n",
    "        df[col] = df[col].fillna(float(1))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_coupon_list_train = update_dates(eng_coupon_list_train, 'train')\n",
    "updated_coupon_list_test = update_dates(eng_coupon_list_test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have clean training and test sets of coupons. I want to examine if there is duplicate coupon in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine if there is a coupon in both training and test sets\n",
    "cnt = 0\n",
    "for coupon in file_dict['coupon_list_test']['COUPON_ID_hash']:\n",
    "    if coupon in file_dict['coupon_list_train']['COUPON_ID_hash']:\n",
    "        cnt += 1\n",
    "\n",
    "print(f'There are {cnt} coupons of test are also in training set',cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting records of visits with no purchases as negative label(0: not purchase)\n",
    "tmp_coupon_visit = file_dict['coupon_visit_train']\n",
    "tmp_coupon_visit_no_purchase = tmp_coupon_visit[tmp_coupon_visit['PURCHASE_FLG']==0][[\"USER_ID_hash\", \"VIEW_COUPON_ID_hash\", \"I_DATE\"]]\n",
    "tmp_coupon_visit_no_purchase.rename(columns={'VIEW_COUPON_ID_hash':'COUPON_ID_hash'},inplace=True)\n",
    "tmp_coupon_visit_no_purchase['label'] = 0\n",
    "\n",
    "#selecting columns from coupon purchase table as positive label(1: purchase)\n",
    "tmp_coupon_purchase = file_dict['coupon_detail_train'][[\"USER_ID_hash\", \"COUPON_ID_hash\", \"I_DATE\"]]\n",
    "tmp_coupon_purchase['label'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate user coupon pair with label\n",
    "user_coupon_train_pair = pd.concat([tmp_coupon_visit_no_purchase,tmp_coupon_purchase],\n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the above dataframe with user features on user id\n",
    "user_coupon_train_pair = user_coupon_train_pair.merge(user_list_filtered, on='USER_ID_hash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then join coupon features on couple id\n",
    "user_coupon_train_pair = user_coupon_train_pair.merge(updated_coupon_list_train, on='COUPON_ID_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_coupon_train_pair.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will pre-processing the input features like one-hot encoding categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process function\n",
    "def pre_process_features(pair_df):\n",
    "    #parsing the view date\n",
    "    pair_df['I_DATE'] = pair_df['I_DATE'].apply(lambda x: parse_date_str(x, True))\n",
    "    \n",
    "    #create a new column which indicates whether the view date is within the discount price period\n",
    "    pair_df['within_dispPeriod'] = pair_df.apply(lambda x:float(1) if x['DISPFROM'] <= x['I_DATE'] <= x['DISPEND'] else float(0), axis=1)\n",
    "\n",
    "    #create a new column indicating whether the view date is within the coupon's valid period\n",
    "    pair_df['within_validPeriod'] = pair_df.apply(lambda x:float(1) if x['VALIDFROM'] <= x['I_DATE'] <= x['VALIDEND'] else float(0), axis=1)\n",
    "    \n",
    "    #create a new column indicating whether this coupon is in the same area of user's residential prefecture\n",
    "    pair_df['usr_cpn_same_area'] = pair_df.apply(lambda x: float(1) if (x['PREF_NAME']=='Unknown' or x['PREF_NAME']==x['ken_name']) else float(0), axis=1)\n",
    "\n",
    "    #create users' age groups \n",
    "    pair_df['age_group'] = pd.cut(pair_df['AGE'],\n",
    "                                bins=[0,22,34, 44, 64,120],\n",
    "                                labels=['Youth', 'Young Adult','Early Mid-age','Late Mid-age','Senior Adult'],\n",
    "                                right=False)\n",
    "    return pair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_coupon_train_pair = pre_process_features(user_coupon_train_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dataframe by date of view or purchase for cross validation split on a rolling basis\n",
    "user_coupon_train_pair.sort_values(by='I_DATE', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "X_categorical = user_coupon_train_pair[['SEX_ID', 'capsule_text_eng','age_group']]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "X_categorical_encoded = one_hot_encoder.fit_transform(X_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric features\n",
    "dense_features = ['num_year_registered','PRICE_RATE',\n",
    "                    'DISCOUNT_PRICE','DISPPERIOD','VALIDPERIOD','USABLE_DATE_MON',\n",
    "                    'USABLE_DATE_TUE','USABLE_DATE_WED','USABLE_DATE_THU',\n",
    "                   'USABLE_DATE_FRI','USABLE_DATE_SAT','USABLE_DATE_SUN',\n",
    "                  'USABLE_DATE_HOLIDAY','USABLE_DATE_BEFORE_HOLIDAY','within_dispPeriod',\n",
    "                    'within_validPeriod','usr_cpn_same_area']\n",
    "X_dense = user_coupon_train_pair[dense_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge numeric and categorical features together\n",
    "X_train_all = np.hstack((X_dense.values,X_categorical_encoded.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tscv = TimeSeriesSplit(n_splits=3)\n",
    "#for train_index, validation_index in tscv.split(X_train):\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take index from user_coupon_train_pair for training and validation \n",
    "train_index = np.where(user_coupon_train_pair['I_DATE']< datetime.strptime('2012-04-01 11:59:00', '%Y-%m-%d %H:%M:%S'))[0]\n",
    "val_index = np.where(user_coupon_train_pair['I_DATE']>= datetime.strptime('2012-04-01 11:59:00', '%Y-%m-%d %H:%M:%S'))[0]\n",
    "\n",
    "#training_perc = len(train_index[0])/len(user_coupon_train_pair)\n",
    "#print('percentage of training data: ',training_perc)\n",
    "#print('percentage of validation data: ',1-training_perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply index to X_train\n",
    "y_train_all = user_coupon_train_pair['label']\n",
    "X_train = X_train_all[train_index]\n",
    "y_train = y_train_all[train_index]\n",
    "X_val = X_train_all[val_index]\n",
    "y_val = y_train_all[val_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=500, max_depth=7,class_weight='balanced', n_jobs=-1)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_val, rfc.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "#feval=rmsle,\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_val, y_val)\n",
    "evals=[(dtrain, 'dtrain'), (dtest, 'dtest')]\n",
    "# specify parameters via map\n",
    "param = {'max_depth':5, 'eta':.1, 'objective':'binary:logistic', 'verbosity':2, \"eval_metric\": \"auc\" }\n",
    "num_round = 200\n",
    "bst = xgb.train(param, dtrain, num_round, evals=evals )\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a user_coupon_test_pair by cross joining clean user list and clean coupon test list\n",
    "\n",
    "user_coupon_test_pair = pd.merge(user_list_filtered, updated_coupon_list_test,\n",
    "                                on='key').drop('key',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume all users have viewed all test coupons on the start date of test period\n",
    "user_coupon_test_pair['I_DATE'] = '2012-06-24 12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(user_coupon_test_pair)\n",
    "test_pair_dict = {}\n",
    "\n",
    "for i in range(50000,n,50000):\n",
    "    last_i = i-50000\n",
    "    if i <= 6800000:\n",
    "        test_pair_dict[(last_i,i)] = pre_process_features(user_coupon_test_pair.iloc[last_i:i,:])\n",
    "\n",
    "test_pair_dict[(i,n)] = pre_process_features(user_coupon_test_pair.iloc[i:,:])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(user_coupon_test_pair)\n",
    "batch_size = 50000\n",
    "user_coupon_dict = defaultdict(list)\n",
    "#res = pd.DataFrame(columns=['USER_ID_hash','COUPON_ID_hash','preds'])\n",
    "results = defaultdict(list)\n",
    "\n",
    "for i in range(n // batch_size+1):\n",
    "    start = i * batch_size\n",
    "    end = (i+1) * batch_size\n",
    "    batch_test_pair = pre_process_features(user_coupon_test_pair.iloc[start:end,:].copy())\n",
    "    #extract the user_id and coupon_id\n",
    "    user_coupon_id = user_coupon_test_pair[['USER_ID_hash','COUPON_ID_hash']][start:end]\n",
    "    \n",
    "    #one-hot encoding    \n",
    "    batch_test_pair_encoded = one_hot_encoder.transform(batch_test_pair[categorical_features])\n",
    "\n",
    "    #numeric features\n",
    "    batch_test_pair_dense = batch_test_pair[dense_features]\n",
    "\n",
    "    #merge numeric and categorical features together\n",
    "    batch_X_test = np.hstack((batch_test_pair_dense.values,batch_test_pair_encoded.toarray()))\n",
    "\n",
    "    #predicting\n",
    "    batch_test_dmatrix = xgb.DMatrix(batch_X_test, label=None)\n",
    "    predictions = bst.predict(batch_test_dmatrix)\n",
    "    \n",
    "    #combine user_id, coupon_id and corresponding predictions\n",
    "    temp_res = user_coupon_id.assign(preds = predictions)\n",
    "    \n",
    "    for index,row in temp_res.iterrows():\n",
    "        results[row['USER_ID_hash']].append((row['COUPON_ID_hash'],row['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.apply(lambda x: sort(x['']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lst in results.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_features = ['SEX_ID', 'capsule_text_eng','age_group']\n",
    "pred_dict = {}\n",
    "for key, batch_test_pair in test_pair_dict.items():\n",
    "    #one-hot encoding    \n",
    "    batch_test_pair_encoded = one_hot_encoder.transform(batch_test_pair[categorical_features])\n",
    "\n",
    "    #numeric features\n",
    "    batch_test_pair_dense = batch_test_pair[dense_features]\n",
    "\n",
    "    #merge numeric and categorical features together\n",
    "    X_test_batch = np.hstack((batch_test_pair_dense.values,batch_test_pair_encoded.toarray()))\n",
    "\n",
    "#predicting\n",
    "    test_dmatrix = xgb.DMatrix(X_test_batch, label=None)\n",
    "    pred_dict[key] = bst.predict(test_dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(pred_dict.values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
